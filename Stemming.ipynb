{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37e4fbf",
   "metadata": {},
   "source": [
    "## Stemming in NLTK  \n",
    "Stemming is a process of reducing word to its root word that affixes to suffixes and prefixes.  \n",
    "It is important for Natural Language Understanding (NLU) and Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9405c4",
   "metadata": {},
   "source": [
    "Let's say i have words like -  \n",
    "- eat, eating, eaten,  \n",
    "- go, going, gone  \n",
    "\n",
    "All these words have root word as \"eat\" and \"go\" respectively. So Stemming is done to find those root words which crucial to form a context. Beacuse at the end of the day, all other forms of the root words are just only increasing the parameters of the model. It tries to get common base form without forming the real dictionary.  \n",
    "The more the parameters of the model, the more it takes to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe44310",
   "metadata": {},
   "source": [
    "It is good for classification problems like Positive feedback or Negative feedback for a product, Email is Spam or Ham, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80790dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"go\", \"going\", \"gone\", \"eat\", \"eating\", \"eaten\", \"programming\", \"fairly\", \"fairness\", \"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de33e79",
   "metadata": {},
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5b8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go ---> go\n",
      "going ---> go\n",
      "gone ---> gone\n",
      "eat ---> eat\n",
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "programming ---> program\n",
      "fairly ---> fairli\n",
      "fairness ---> fair\n",
      "history ---> histori\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} ---> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79babc26",
   "metadata": {},
   "source": [
    "When stemming is applied, for some of the words, we won't get the correct word. For example:  \n",
    "- fairly ---> fairli\n",
    "- history ---> histori  \n",
    "\n",
    "This is the major problem with stemming because it has changed the entire meaning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51deb9",
   "metadata": {},
   "source": [
    "#### RegexpStemmer  \n",
    "NLTK has RegexpStemmer class which helps us to implement Regular Expression Stemmer Algorithms. It takes a Regular Expression and removes any suffixes or prefixes present in the word that matches the expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15a5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go --> go\n",
      "going --> go\n",
      "gone --> gone\n",
      "eat --> eat\n",
      "eating --> eat\n",
      "eaten --> eaten\n",
      "programming --> programm\n",
      "fairly --> fair\n",
      "fairness --> fair\n",
      "history --> history\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer(\"ing$|s$|e$|able$|ed$|ness$|ly$\", min = 5)\n",
    "\n",
    "## use of min parameter is to avoid stemming small words like \"go\", \"eat\" etc.\n",
    "## example: \"go\" ---> \"g\" if min is not used. if min = 5, it will not stem \"go\" because its length is less than 5.\n",
    "\n",
    "for word in words:\n",
    "    ans = reg_stemmer.stem(word)\n",
    "    print(f\"{word} --> {ans}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd0088",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6688528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27b67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go ---> go\n",
      "going ---> go\n",
      "gone ---> gone\n",
      "eat ---> eat\n",
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "programming ---> program\n",
      "fairly ---> fair\n",
      "fairness ---> fair\n",
      "history ---> histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} ---> {snow_stemmer.stem(word)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
